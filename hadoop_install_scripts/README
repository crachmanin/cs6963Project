1. Copy the following scripts from the user machine to the controller(check the ssh command in list view in Openstack)
   scp latest_working/master_script.sh unniar@pc704.emulab.net:
   scp latest_working/install_hadoop.sh unniar@pc704.emulab.net:
   scp latest_working/hadoop_master_conf.tar.gz unniar@pc704.emulab.net:
   scp latest_working/hadoop_resourcemanager_conf.tar.gz unniar@pc704.emulab.net:
   scp latest_working/hadoop_slaves_conf.tar.gz unniar@pc704.emulab.net:
   scp latest_working/install_hadoop_resourcemanager.sh unniar@pc704.emulab.net:
   scp latest_working/install_hadoop_slave0.sh unniar@pc704.emulab.net:
   scp latest_working/install_hadoop_slave1.sh unniar@pc704.emulab.net:
   scp latest_working/install_hadoop_slave2.sh unniar@pc704.emulab.net:
   scp latest_working/resourcemanager_script.sh unniar@pc704.emulab.net:
   scp latest_working/slave0_script.sh unniar@pc704.emulab.net:
   scp latest_working/slave1_script.sh unniar@pc704.emulab.net:
   scp latest_working/slave2_script.sh unniar@pc704.emulab.net:

   This contains the preconfigured tarballs for master, slave and resourcemanager instances
   

2. Start login_conf in your machine(not the controller)with following parameters
   ip address of master(master VM in the Openstack)
   pwd for openstack(password generated when the project gets created)
   eg:
   unni@Unni:./login_conf 10.11.10.9 67719671246d

   The script will end in the controller home directory
   check the scripts have been copied previously and hadoop binary is downloaded on the controller

3. Run the master_script with the following parameters
   ip master in openstack (10.11.10.9)
   ip slave0 in openstack(10.11.10.10)
   ip slave1 in openstack(10.11.10.11)
   ip slave2 in openstack(10.11.10.12)
   ip resourcemanager in openstack(10.11.10.13)
   eg:
   ./master_script 10.11.10.9 10.11.10.10 10.11.10.11 10.11.10.12 10.11.10.13

   The script ends with the shell prompt in the master
   Check whether /etc/hosts has been updated properly with entries of master, slaves and resourcemanager
   delete "127.0.0.1 localhost" master if it exists in the beginning because there might be duplicate entries

   exit from the master after checking the /etc/hosts have been modified properly
   eg:
   unniar@master:~$ exit

4. run install_hadoop.sh script with follwoing parameter
   ip master
   pwd for openstack
   eg:
   ./install_hadoop.sh 10.11.10.9(ip of openstack master) 67719671246d 
    
   This script will configure hadoop on the master and add ssh keys on the slaves
   Check the master node shell prompt and check whether the scripts for the slaves and resourcemanager have been copied

5. From the master hadoop node run the following scripts to configure the slaves and resourcemanager
   -> Run slave0_script with same parameters used for running the master script
   ./slave0_script.sh 10.11.10.9 10.11.10.10 10.11.10.11 10.11.10.12 10.11.10.13
    check /etc/hosts for duplicate copy of "127.0.0.1 localhost" and delete it if there are duplicate copies
    exit

   ./slave1_script.sh 10.11.10.9 10.11.10.10 10.11.10.11 10.11.10.12 10.11.10.13
     check /etc/hosts for duplicate copy of "127.0.0.1 localhost" and delete it if there are duplicate copies
     exit

   ./slave2_script.sh 10.11.10.9 10.11.10.10 10.11.10.11 10.11.10.12 10.11.10.13
     check /etc/hosts for duplicate copy of "127.0.0.1 localhost" and delete it if there are duplicate copies
     exit

   ./resourcemanager_script.sh 10.11.10.9 10.11.10.10 10.11.10.11 10.11.10.12 10.11.10.13
     check /etc/hosts for duplicate copy of "127.0.0.1 localhost" and delete it if there are duplicate copies
     exit

   The following scripts will add the necessary tools and add the ip addresses in the /etc/hosts file
   check /etc/hosts for duplicate copy of "127.0.0.1 localhost" and delete it if there are duplicate copies

6. From the master node run the following scripts to configure hadoop

   Pass the appropriate ip address of the hosts in the Openstack VM's to the scripts

   ./install_hadoop_slave0.sh 10.11.10.10(slave0 in openstack) 67719671246d

   ./install_hadoop_slave1.sh 10.11.10.11(slave1 in openstack) 67719671246d

   ./install_hadoop_slave2.sh 10.11.10.12(slave2 in openstack) 67719671246d

   ./install_hadoop_slave3.sh 10.11.10.13(resourcemanager in openstack) 67719671246d

7. In the master node check the hadoop-2.7.1 folder

   cd into logs and check whether its empty   

   check whether the configurations has been copied properly in the etc/hadoop folder

8. cd into the sbin folder in hadoop-2.7.1
  
   run the start-hdfs.sh (./start-dfs.sh)

   check the processes which have been started

   sudo jps

   This should output the following processes(this depends if you make the namenode as the datanode too)

   namenode
   datanode
   secondarynamenode

   if you don't see the namenode starting, probably it could be a locking issue

   check the master node logs in logs folder

   stop any instances if they are running with the following script
   
   ./stop-dfs.sh

   format the namenode with the following command

   /bin/hadoop namenode -format

   run the hdfs script again

   ./start-dfs.sh

   check the output and if the output is as per above then start the yarn else check the logs to debug

   ./start-yarn.sh

   Running jps will output the following

   namenode
   datanode
   nodetracker
   secondarynamenode

   login into the slaves and run jps 

   this should ouput the following
   
   nodetracker
   datanode

   Run a sample mapreduce(calculating pi) job to see hadoop runs fine
   
   cd into bin folder in hadoop home(hadoop-2.7.1 folder)

   hadoop jar /home/ubuntu/hadoop-2.7.1/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.4.0.jar pi 30 100

   check the hadoop-mapreduce-examples-2.4.0.jar for which version is shipped in the distribution
   

